{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eaCAV4hws0jj"
   },
   "source": [
    "## Working with embedding layers and 1d convolution layers \n",
    "- examples using sequential models\n",
    "- demos of dropout for sequential models and bidirectional sequential layers along the way\n",
    "\n",
    "Code adjusted from *Deep Learning for Python* Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "VSGPkI6Lrhyq",
    "outputId": "fda42716-cc4d-42c2-e243-e7367552f6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-04 13:44:37--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  18.2MB/s    in 8.4s    \n",
      "\n",
      "2020-05-04 13:44:46 (9.56 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get raw imdb dataset\n",
    "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJMsKgAPr5to"
   },
   "outputs": [],
   "source": [
    "# Untar it to a new folder\n",
    "! tar xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPpeq5YosWrH"
   },
   "outputs": [],
   "source": [
    "# Build corpus of docs and labels\n",
    "import os\n",
    "\n",
    "imdb_dir = 'aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "fAystOcWsfBD",
    "outputId": "b6d8cdc5-4323-450b-d21c-5dc257f3fd27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an avid B-Rate horror film buff and have viewed my fair share of slasher pictures, so I have a substantial gauge to judge this film by. It easily ranks in the upper echelon of the worst horror films the 1980's has to offer. It isn't as scary as Night of the Demons, it isn't as gory as Re-Animator and lacks the camp value of There's Nothing Out There. That being said, this film has no value. Keep in mind, the movie artwork is for a completely different film. The stills shots on the back of the DVD box aren't taken from this film.<br /><br />VIOLENCE: $$$ (There is plenty of violence but we've seen it all before. A murderer kills nubile students and the occasional facility member by slitting throats and all the other tired methods of murder that horror films utilize).<br /><br />NUDITY: None <br /><br />STORY: $$ (The story focuses on Francine Forbes - who wisely changed her name to Forbes Riley after this film was made - who accepts a job teaching at a university. People start to die and Forbes believes the killer is targeting her. Is it her new heartthrob with a checkered past or the libido-crazed student? To be honest, it is impossible to care because the script doesn't flesh out any character outside Forbes).<br /><br />ACTING: $ (Terrible on all levels. This slasher has the feel of a school production -high school that is because college students could make a better flick than this. Forbes showcases a modicum of talent as does Seminara as one of the students, but everyone else is of the \"extras\" caliber of acting).\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "A4kmpHZ1slPV",
    "outputId": "1115ba1a-09dc-40e2-8472-3c42ebe76baf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data into one hot vectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100  # We will cut reviews after 100 words\n",
    "training_samples = 200  # We will be training on 200 samples\n",
    "validation_samples = 10000  # We will be validating on 10000 samples\n",
    "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) # converts words in each text to each word's numeric index in tokenizer dictionary.\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "# But first, shuffle the data, since we started from data\n",
    "# where sample are ordered (all negative first, then all positive).\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples] #200 words\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "txjBFVQqua5f",
    "outputId": "64b0a43d-6d1f-466f-ef80-62fe5bdba3ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 8)            80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 801       \n",
      "=================================================================\n",
      "Total params: 80,801\n",
      "Trainable params: 80,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6939 - acc: 0.5000 - val_loss: 0.6884 - val_acc: 0.6250\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.6701 - acc: 0.8438 - val_loss: 0.6882 - val_acc: 0.6250\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 113us/step - loss: 0.6526 - acc: 0.9563 - val_loss: 0.6880 - val_acc: 0.6250\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.6359 - acc: 0.9750 - val_loss: 0.6877 - val_acc: 0.6000\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.6189 - acc: 0.9812 - val_loss: 0.6874 - val_acc: 0.5750\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.6008 - acc: 0.9937 - val_loss: 0.6871 - val_acc: 0.6000\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 103us/step - loss: 0.5820 - acc: 0.9937 - val_loss: 0.6866 - val_acc: 0.6000\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 124us/step - loss: 0.5619 - acc: 0.9937 - val_loss: 0.6861 - val_acc: 0.6000\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.5411 - acc: 0.9937 - val_loss: 0.6856 - val_acc: 0.6250\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.5192 - acc: 0.9937 - val_loss: 0.6852 - val_acc: 0.6250\n"
     ]
    }
   ],
   "source": [
    "# Let's start with a model that ignores the sequential steps that make up each observation\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "# Specify the size of your vocabulary (i.e.-10,000 terms)\n",
    "# Specify the number of features you want to extract via fitting weights to your embedding matrix.\n",
    "# We also specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs \n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings \n",
    "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add the classifier on top\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "950gVSU0vzZk",
    "outputId": "a505d562-0ce6-4365-e2e7-538e442081c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-04 13:45:07--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
      "--2020-05-04 13:45:08--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
      "--2020-05-04 13:45:08--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182753 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.14MB/s    in 6m 29s  \n",
      "\n",
      "2020-05-04 13:51:38 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What if we wanted to use a matrix of pretrained embeddings?  Same as transfer learning before, but now we are importing a pretrained Embedding matrix:\n",
    "# Download Glove embedding matrix weights (Might take 10 mins or so!)\n",
    "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "xHDZ80sAyWFR",
    "outputId": "e419e966-a2e3-4327-f9df-f882366cad46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n",
      "  inflating: glove.6B.50d.txt        \n"
     ]
    }
   ],
   "source": [
    "! unzip glove.6B.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "sdPAVxLcw0zo",
    "outputId": "e1ec2806-123f-44ce-82db-561184331024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Extract embedding data for 100 feature embedding matrix\n",
    "glove_dir = os.getcwd()\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYAaB4Z9w55d"
   },
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_dim = 100 # change if you use txt files using larger number of features\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "Lpqy1QJqxI0m",
    "outputId": "54cf5a67-0a20-4e9f-9473-527806d0c6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set up same model architecture as before and then import Glove weights to Embedding layer:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "K-GB3K1uzFU1",
    "outputId": "3f1a25c8-56b8-4499-f6f3-5d04c0e9bf89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.4497 - acc: 0.5050 - val_loss: 0.7789 - val_acc: 0.5032\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5007 - acc: 0.7650 - val_loss: 1.2360 - val_acc: 0.5042\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4402 - acc: 0.8350 - val_loss: 0.8863 - val_acc: 0.5026\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4775 - acc: 0.7550 - val_loss: 1.1220 - val_acc: 0.4960\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2486 - acc: 0.9300 - val_loss: 0.8481 - val_acc: 0.5065\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1726 - acc: 0.9950 - val_loss: 0.9987 - val_acc: 0.5024\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1658 - acc: 0.9850 - val_loss: 0.7854 - val_acc: 0.5096\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0875 - acc: 1.0000 - val_loss: 0.7767 - val_acc: 0.5045\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0874 - acc: 0.9950 - val_loss: 0.8155 - val_acc: 0.5036\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0460 - acc: 1.0000 - val_loss: 0.8294 - val_acc: 0.5087\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add weights in same manner as transfer learning and turn of trainable option before fitting model to freeze weights.\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')\n",
    "\n",
    "\n",
    "# Training data small to speed up training. Increase for better fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHYhpu9Izfh1"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test set (need to preprocess test data to same structure first)\n",
    "\n",
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "#using tokenizer object we fit to test data above\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "kqorhBl7z4GW",
    "outputId": "823b807f-3c18-4434-f1a0-fbcaffe32c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 44us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8142335695266724, 0.5177599787712097]"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "\n",
    "model.evaluate(x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQhhrKWK0vz8"
   },
   "source": [
    "## Fitting sequential models to text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "NR73_90Rz72g",
    "outputId": "c8b6b496-6fc4-4fc4-d0ad-f5ccc6867f03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6996 - acc: 0.5000 - val_loss: 0.7073 - val_acc: 0.4000\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 726us/step - loss: 0.5300 - acc: 0.9187 - val_loss: 0.7142 - val_acc: 0.4750\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 787us/step - loss: 0.4217 - acc: 0.9563 - val_loss: 0.7256 - val_acc: 0.4000\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 747us/step - loss: 0.2712 - acc: 1.0000 - val_loss: 0.8405 - val_acc: 0.3500\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 729us/step - loss: 0.1914 - acc: 1.0000 - val_loss: 0.7606 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 707us/step - loss: 0.1621 - acc: 1.0000 - val_loss: 0.9073 - val_acc: 0.3750\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 704us/step - loss: 0.1365 - acc: 1.0000 - val_loss: 0.8559 - val_acc: 0.3500\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 813us/step - loss: 0.0912 - acc: 1.0000 - val_loss: 0.9057 - val_acc: 0.3000\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 794us/step - loss: 0.0704 - acc: 1.0000 - val_loss: 0.9799 - val_acc: 0.3250\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 704us/step - loss: 0.0570 - acc: 1.0000 - val_loss: 0.9527 - val_acc: 0.3750\n"
     ]
    }
   ],
   "source": [
    "# Example 1: simple RNN\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "# Small training data.  Increase for model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "Xl_uowG25kae",
    "outputId": "e2bc8380-8cab-4cc1-e87d-c1e99efaffd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 7ms/step - loss: 0.7390 - acc: 0.4500 - val_loss: 0.7176 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5616 - acc: 0.7625 - val_loss: 0.8138 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.2477 - acc: 0.9625 - val_loss: 0.9163 - val_acc: 0.3750\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.0842 - acc: 1.0000 - val_loss: 0.9652 - val_acc: 0.4500\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.0409 - acc: 1.0000 - val_loss: 1.0614 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.0255 - acc: 1.0000 - val_loss: 1.0802 - val_acc: 0.4250\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 1.1149 - val_acc: 0.4500\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 1.1568 - val_acc: 0.4500\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 1.1836 - val_acc: 0.4750\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 1.2167 - val_acc: 0.4750\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Stacked RNN layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "# Small training data.  Increase for model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "2gxDdKbi53z7",
    "outputId": "40d47950-3167-45a8-f052-e915d317ae26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6920 - acc: 0.5625 - val_loss: 0.6952 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6789 - acc: 0.5625 - val_loss: 0.6972 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6618 - acc: 0.5625 - val_loss: 0.7067 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5954 - acc: 0.5750 - val_loss: 1.1216 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5686 - acc: 0.7563 - val_loss: 0.7647 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.4072 - acc: 0.7875 - val_loss: 0.6794 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.3419 - acc: 0.9500 - val_loss: 0.8986 - val_acc: 0.5250\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.2731 - acc: 0.9875 - val_loss: 1.0233 - val_acc: 0.5500\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.2891 - acc: 0.9937 - val_loss: 0.8689 - val_acc: 0.6250\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.2185 - acc: 1.0000 - val_loss: 0.8765 - val_acc: 0.6500\n"
     ]
    }
   ],
   "source": [
    "# Example 3: LSTM layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "# Small training data.  Increase for model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "Ql2SgghF9n-q",
    "outputId": "141ff520-3769-448b-ff6c-9dd9e0f40a2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 7ms/step - loss: 0.6945 - acc: 0.4688 - val_loss: 0.6948 - val_acc: 0.4000\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6870 - acc: 0.7812 - val_loss: 0.6999 - val_acc: 0.3500\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6815 - acc: 0.6000 - val_loss: 0.7051 - val_acc: 0.3500\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6713 - acc: 0.5375 - val_loss: 0.7139 - val_acc: 0.3500\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6583 - acc: 0.5188 - val_loss: 0.7277 - val_acc: 0.3500\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6358 - acc: 0.5188 - val_loss: 0.7526 - val_acc: 0.3500\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5859 - acc: 0.6500 - val_loss: 0.9989 - val_acc: 0.3500\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5750 - acc: 0.6375 - val_loss: 0.6843 - val_acc: 0.6500\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5380 - acc: 0.9438 - val_loss: 0.6921 - val_acc: 0.5750\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.4598 - acc: 0.9500 - val_loss: 0.7495 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "#Example 4: Bidirectional LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAHcMGPu6b5l"
   },
   "source": [
    "### Some Dropout Examples for LSTM layers\n",
    "Dropout was a difficult puzzle for sequential models.  Solved relatively recently by dropping out on same hidden node locations at each time step.\n",
    "\n",
    "LSTM(128, dropout=0.2, recurrent_dropout=0.2)) \n",
    "\n",
    "    dropout: Float between 0 and 1.  \n",
    "        Fraction of the units to drop for  \n",
    "        the linear transformation of the inputs.  \n",
    "    recurrent_dropout: Float between 0 and 1.  \n",
    "        Fraction of the units to drop for  \n",
    "        the linear transformation of the recurrent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "colab_type": "code",
    "id": "VCSijr4L5wbg",
    "outputId": "8e1fa0af-c226-46b1-f101-f43c56d2b3ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 7ms/step - loss: 0.6916 - acc: 0.5000 - val_loss: 0.6946 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6820 - acc: 0.5625 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7992 - acc: 0.5625 - val_loss: 0.6951 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6529 - acc: 0.5625 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6334 - acc: 0.5875 - val_loss: 0.6952 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.5920 - acc: 0.6562 - val_loss: 0.6905 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6054 - acc: 0.8313 - val_loss: 0.6833 - val_acc: 0.5500\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.5206 - acc: 0.9937 - val_loss: 0.6712 - val_acc: 0.6250\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.3699 - acc: 0.9500 - val_loss: 0.5716 - val_acc: 0.7500\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.3104 - acc: 0.9250 - val_loss: 0.6969 - val_acc: 0.5750\n",
      "25000/25000 [==============================] - 24s 958us/step\n",
      "Test score: 0.6887399098205567\n",
      "Test accuracy: 0.5476800203323364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOAuHo3v7QmZ"
   },
   "source": [
    "## Sequential models using 1D Convnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "hakEYjpW7Gws",
    "outputId": "580e3594-52b2-4e8e-c96e-0907ec26e925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 500)\n",
      "x_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000  # number of words to consider as features\n",
    "max_len = 500  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "colab_type": "code",
    "id": "HAjse-QU7bNi",
    "outputId": "0215e229-bdd5-4dba-c863-c0f15e949995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 500, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 71s 4ms/step - loss: 0.7465 - acc: 0.5114 - val_loss: 0.6882 - val_acc: 0.5334\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 72s 4ms/step - loss: 0.6674 - acc: 0.6584 - val_loss: 0.6661 - val_acc: 0.6488\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 72s 4ms/step - loss: 0.6242 - acc: 0.7564 - val_loss: 0.6144 - val_acc: 0.7392\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 71s 4ms/step - loss: 0.5335 - acc: 0.8098 - val_loss: 0.4927 - val_acc: 0.7974\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 71s 4ms/step - loss: 0.4131 - acc: 0.8504 - val_loss: 0.4127 - val_acc: 0.8354\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 74s 4ms/step - loss: 0.3401 - acc: 0.8689 - val_loss: 0.3998 - val_acc: 0.8392\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 80s 4ms/step - loss: 0.2999 - acc: 0.8730 - val_loss: 0.3837 - val_acc: 0.8388\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.2656 - acc: 0.8654 - val_loss: 0.3893 - val_acc: 0.8272\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.2427 - acc: 0.8516 - val_loss: 0.3992 - val_acc: 0.8086\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 81s 4ms/step - loss: 0.2190 - acc: 0.8339 - val_loss: 0.4214 - val_acc: 0.7782\n"
     ]
    }
   ],
   "source": [
    "# Use 1D Conv layer rather than RNN or LSTM or GRU to fit model\n",
    "# Why? Much lighter model to fit. Here we are training on the full dataset.  If you try\n",
    "# to build a model using LSTM code after running this one it will be much slower.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu')) \n",
    "model.add(layers.MaxPooling1D(5)) #\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM for Sentiment Analysis code in progress use me.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
